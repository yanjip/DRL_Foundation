# DDPG(4.21)
- Pendulum-v1的动作空间是一个连续的一维空间，表示施加在摆杆自由端的扭矩，范围是[-2.0, 2.0]12。Pendulum-v1的观察空间是一个连续的三维空间，
表示摆杆自由端的x-y坐标和角速度，范围分别是[-1.0, 1.0]，[-1.0, 1.0]和[-8.0, 8.0]12
- 软更新系数0.005;
高斯噪声标准差0.01
- 策略网络-价值网络（分别对应两个target网络）
- 激活函数最后一层使用的是tanh，调整数值到【-1,1】，然后乘以边界值即可。
- 在演员 (Actor) 网络中，需要输出连续动作;评论家 (Critic) 网络需要将状态和动作拼接起来作为其输入
来估计Q值，并用Q值来更新演员网络。
- target_update:2
- replay buffer可以放在DDPG对象里面，也可以单独拿出来
- take action函数，记得detach一下action的值


- 重要：网络更新流程
    - 价值网络更新：
    - next_action=target_actor(next_state)
    - target_value=target_critic(next_state,next_acion.detach())
    - TD_target=reward + (1.0 - done) * self.gamma * target_value
    - now_value=critic(state, action)
    -  value_loss = nn.MSELoss()(now_value, TD_target.detach())
    - 策略网络更新：
    - now_actor=actor(state)
    - score_now=critic(state, now_actor)
    - policy_loss = -score_now.mean()或-torch.mean(now_score)
    
# PPO(4.21)
网络更新
- td_value =critic(states)
- q_next_state=critic(next_states)
- td_target = rewards + self.gamma * q_next_state * (1 - dones)
- td_delta = (td_target - td_value).detach().numpy
- 


